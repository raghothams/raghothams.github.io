<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>technology on Raghotham Sripadraj</title>
    <link>https://raghothams.github.io/tags/technology/</link>
    <description>Recent content in technology on Raghotham Sripadraj</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Sat, 21 Mar 2020 19:58:42 +0530</lastBuildDate>
    
	<atom:link href="https://raghothams.github.io/tags/technology/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Fontastic - Part I</title>
      <link>https://raghothams.github.io/posts/fontastic-data-acquisition/</link>
      <pubDate>Sat, 21 Mar 2020 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/fontastic-data-acquisition/</guid>
      <description>In the previous post, we spoke about why Nischal and I started Fontastic as a side project. However, a year down the line, we realized there were major learnings for us while working on the project and this could be useful to many others as well. Hence, we decided to share our learnings through a series of posts and we start with data acquistion as our first story.
In this post we cover the various data acquisition strategies we had to experiment and their pros and cons.</description>
    </item>
    
    <item>
      <title>Generalists</title>
      <link>https://raghothams.github.io/posts/generalists/</link>
      <pubDate>Mon, 20 Jan 2020 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/generalists/</guid>
      <description>Traditionally, machine learning roles (in 2000s) were taken up by people mostly from research background. Their go-to toolkit would generally be R or Matlab because of the richness of algorithms offered by these programming languages. These solutions were intended to run on single compute instances. Slowly, these roles were taken by brilliant engineers who strongly believed in open source philosophy. They built algorithms in python which were easy to use and more accessible.</description>
    </item>
    
    <item>
      <title>Deep Learning for Fonts</title>
      <link>https://raghothams.github.io/posts/deep-learning-fonts/</link>
      <pubDate>Sun, 16 Dec 2018 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/deep-learning-fonts/</guid>
      <description>2016, It was a sunny morning in London, Nischal &amp;amp; I were roaming around the Westminster Bridge.
We were speaking at PyData London that year and the topic was Deep Learning
Of all the things, there was this one thing we were super curious to know — Which is the font on the boards of the buses?
Picture courtesy — Wikipedia
This was one of the most legible, crisp, clean font we had ever seen!</description>
    </item>
    
    <item>
      <title>MongoDB and PySPark 2.1.0</title>
      <link>https://raghothams.github.io/posts/mongodb-pyspark/</link>
      <pubDate>Wed, 12 Dec 2018 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/mongodb-pyspark/</guid>
      <description>Photo by Thomas Kvistholt on Unsplash
The common problem with using the latest release of any framework is that there are no or very few adopters, docs are not updated or point to older versions. We encountered a similar problem while integrating MongoDB driver with Apache Spark 2.X. Majority of the library docs available as of today work only with spark 1.5+.
All we wanted to do was to create a dataframe by reading a mongodb collection.</description>
    </item>
    
    <item>
      <title>Spark 2.x External Packages</title>
      <link>https://raghothams.github.io/posts/spark-external-packages/</link>
      <pubDate>Tue, 11 Dec 2018 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/spark-external-packages/</guid>
      <description>Photo by Mika Baumeister on Unsplash
The bane of using bleeding edge technology is very less or hidden information of new features in the latest version. We at Unnati use bleeding edge releases of many data science tools for various research and production systems. In this post we explain how to add external jars to Apache Spark 2.x application.
Starting Spark 2.x, we can use the --package option to pass additional jars to spark-submit.</description>
    </item>
    
    <item>
      <title>A Prettier Word Cloud</title>
      <link>https://raghothams.github.io/posts/prettier-word-cloud/</link>
      <pubDate>Sat, 08 Oct 2016 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/prettier-word-cloud/</guid>
      <description>Word cloud is one of the most common visualizations we see today, especially with social media analytics. Open source libraries like D3JS have eased developers life. With these libraries we can quickly wire data and get beautiful visualizations. Thanks to Mike Bostock for giving the community d3.js and http://bl.ocks.org. With bl.ocks, we have a plethora of visualizations from the community, open to public with their implementation.
This library from Jason Davies — https://github.</description>
    </item>
    
    <item>
      <title>Word Embedding</title>
      <link>https://raghothams.github.io/posts/word-embeddings/</link>
      <pubDate>Sat, 10 Oct 2015 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/word-embeddings/</guid>
      <description>Word embedding is a technique of converting words to vectors of a high dimension space. In simple terms, in each dimension, we group words based on a particular aspect — gender, colour etc., and score the words based on similarity in that space.
 For example — “I have a red car, maroon shirt and a grey bicycle”
 One of the dimensions can represent colour. Red, maroon and grey are assigned similar scores.</description>
    </item>
    
    <item>
      <title>Introduction to Neural Networks</title>
      <link>https://raghothams.github.io/posts/intro-neural-networks/</link>
      <pubDate>Tue, 06 Oct 2015 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/intro-neural-networks/</guid>
      <description>Artificial Neural Networks (ANNs) have totally changed what computers are capable of learning. Though neural networks date back 1940s, we are seeing an astonishing amount of increase of its applications in the recent 5–10 years.
Artificial neural networks are modeled on the functioning of the human brain, where the input is converted into output based on a series of transformations. Though they are capable of achieving complex tasks, the way they work is fairly straight forward.</description>
    </item>
    
    <item>
      <title>Parsing JSON in Scala</title>
      <link>https://raghothams.github.io/posts/parsing-json-scala/</link>
      <pubDate>Sun, 26 Apr 2015 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/parsing-json-scala/</guid>
      <description>Introduction I started a side project on Scala with a group of friends (noobs in scala). We chose Scala because it is well known for type safety and functional programming with support for OOP. One of the important parts of the project was speaking to a REST API which returned JSON responses.
We began our hunt for efficient JSON parsers on scala and soon we were flooded with libraries:
 spray-json jerkson jackson json4s jacksMapper  With so many options, we were confused!</description>
    </item>
    
    <item>
      <title>DLNA on RaspberryPi</title>
      <link>https://raghothams.github.io/posts/dlna-raspberrypi/</link>
      <pubDate>Sun, 07 Sep 2014 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/dlna-raspberrypi/</guid>
      <description>I always wanted to setup a media server at home for the following reasons:
 Reduce redundancy — having multiple copies of media for different devices like phone, tablet, smart TV etc Ease of use — no need to copy files to and from devices to play media (mostly Pink Floyd and movies) One stop shop with transmission integration — download files on RaspberryPi and they appear on the media server  The easiest solution was to turn my RaspberryPi into a DLNA server.</description>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>https://raghothams.github.io/posts/machine-learning/</link>
      <pubDate>Wed, 11 Jun 2014 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/machine-learning/</guid>
      <description>I had zero knowledge about machine learning, but wanted to explore. I took up Large Scale Hierarchical Text Classification (LSHTC) as my Masters dissertation project, so that I have a good scenario to start Machine Learning
The first thing I wanted to know was the format of data provided by LSHTC. Turned out that it was SVM format. The training data and test data had the following format
label,label,label… feature:value feature:value</description>
    </item>
    
    <item>
      <title>Text Search on PostgreSQL</title>
      <link>https://raghothams.github.io/posts/text-search-postgres/</link>
      <pubDate>Sat, 31 May 2014 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/text-search-postgres/</guid>
      <description>PostgreSQL has out of box support for text search.
Assume we have a table of documents:
CREATE TABLE documents ( id SERIAL NOT NULL, doc TEXT ) INSERT INTO documents (doc) VALUES (&amp;#34;lorem ipsum .....&amp;#34;); INSERT INTO documents (doc) VALUES (&amp;#34;quick brown fox .....&amp;#34;); Output:
id | doc ----------------- 0 | &amp;#34;Lorem ipsum .....&amp;#34; 1 | &amp;#34;Quick brown fox ...&amp;#34; A simple text search is a basic requirement in any system.</description>
    </item>
    
    <item>
      <title>Database Triggers</title>
      <link>https://raghothams.github.io/posts/database-triggers/</link>
      <pubDate>Tue, 13 May 2014 19:58:42 +0530</pubDate>
      
      <guid>https://raghothams.github.io/posts/database-triggers/</guid>
      <description>Database trigger is an event that can occur after or before a statement is executed or a row is modified / inserted / deleted. This can be used to perform any task before or after certain occurrence of an event in the database.
I was curious about this concept from a very long time and wanted to check it out.
I wanted to try an automation by creating a trigger function.</description>
    </item>
    
  </channel>
</rss>